UNNAMED is a classifier of outlier sequences in multiple alignments

Website:
  https://github.com/alevchuk/ms

Requirements:
  GNU Linux environment
  R
  Ruby
  Bioperl (needed by GUIDANCE)
  dos2unix (needed by GUIDANCE)

Included packages:
  MAFFT v6.857
  GUIDANCE v1.1
  GNU Parallel v20110822

Tested on:
  Debian GNU/Linux 6.0 (Lenny)

Contact:
  Aleksandr Levchuk <alecvhuk@gmail.com>

Installing:
  One command is all it takes. Simpy type: make. Should take about 4 minutes.
  Installs everithing is <project>/opt - the local project directory.

Running the MSA cleanup utility:
  You can use the classifier right away. Simply run the ./008-remover
  and it will print out the usage help.

Running the complete experiment:
  Data, with the exception of the classifier, is not included. Follow 
  data/DATA_README for instruction on downloading versions of CDD and Uniprot.

  Once data tar.gz are downloaded, run each command manually:
  002-start, 004-reshape-cdd, 006-reshape-uniprot, 008-sample, ...

  006-reshape-uniprot is the most time consuming non-parallelizable task,
  it converts FASTA to TAB and takes 7.6 hours. 009-add-random can take 1 hour
  for sample size 100 and 5 samples total. Most steps will need to
  know specifics such as what sample size to operate on. 010-run-guidance and
  026-run-guidance are highly parallel each script will only take a few minutes
  but there are SAMPLE_SIZE * NUM_SAMPLES * (1 + NUM_RANDOM_INJECTIONS) scripts
  which are generated by 010-run-guidance and 026-run-guidance. These scripts
  should preferably run on a cluster via qsub or on a number of local CPU-cores
  via GNU parallel:

  ls ./trial-cdd-2011-XX-uniprot-2011-XX/010-run-guidance-data-in/tasks/* | \
    ./opt/parallel-20110822/bin/parallel

  ls ./trial-cdd-2011-XX-uniprot-2011-XX/026-run-normd-data-in/tasks/* | \
    ./opt/parallel-20110822/bin/parallel

  Both runs above can start immediately after 009-add-random is completed.

Contributing:
  This code is GPLv3. Free Software. So please don't hesitate to make a fork
  on GitHub, make changes, and send GitHub Pull Requests. I will gladly
  accept any revisions and will cite you (the author) as you prefer.

Pipeline API:
  Almost all scripts use ./api/pipeline.sh a very small script to help avoid
  repeating some fine-granularity operations, useful is you forget which
  operations already completed and if some cluster jobs terminate.
